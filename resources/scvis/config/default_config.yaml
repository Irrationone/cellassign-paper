hyperparameter:
  optimization:
    method: Adam
    learning_rate: 0.01
  batch_size: 512
  max_epoch: 100
  regularizer_l2: 0.001
  perplexity: 10.0
  seed: 1
  rel_convergence_thres: 0.0001
  convergence_iters: 3
architecture:
  latent_dimension: 2
  inference:
    layer_size:
    - 128
    - 64
    - 32
  model:
    layer_size:
    - 32
    - 32
    - 32
    - 64
    - 128
  activation: ELU
